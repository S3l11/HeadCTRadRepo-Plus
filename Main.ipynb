{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install language-tool-python\n",
        "!pip install rouge\n",
        "!pip install tensorflow\n",
        "!pip install openpyxl\n",
        "!pip install nltk"
      ],
      "metadata": {
        "id": "cNdT-6_eFV48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "import pickle\n",
        "import string\n",
        "import random\n",
        "import gc\n",
        "import chardet\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "import nltk\n",
        "import re\n",
        "import openpyxl\n",
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu, SmoothingFunction\n",
        "from nltk.translate.meteor_score import meteor_score, single_meteor_score\n",
        "from rouge import Rouge\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Dense, Dropout, LSTM, Concatenate, Reshape,\n",
        "    BatchNormalization, Attention, GlobalAveragePooling1D\n",
        ")\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import load_img, img_to_array, to_categorical, pad_sequences, plot_model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras import mixed_precision\n",
        "from openpyxl.styles import Font, PatternFill\n",
        "from openpyxl.utils.exceptions import IllegalCharacterError\n",
        "from PIL import ImageFile\n",
        "from pickle import dump"
      ],
      "metadata": {
        "id": "1oaXlYHzDiu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(\"Memory growth enabled on the GPU.\")\n",
        "    except RuntimeError as e:\n",
        "        print(e)"
      ],
      "metadata": {
        "id": "hRq3EtRwAnhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_encoding(file_path):\n",
        "    with open(file_path, 'rb') as file:\n",
        "        raw_data = file.read()\n",
        "    result = chardet.detect(raw_data)\n",
        "    encoding = result['encoding']\n",
        "    return encoding\n",
        "\n",
        "def load_caption_file(path):\n",
        "    encoding = detect_encoding(path)\n",
        "    captions_dict = {}\n",
        "    with open(path, \"r\", encoding=encoding) as file:\n",
        "        for line in file:\n",
        "            parts = line.split(\"\\t\\t\")\n",
        "            if len(parts) < 2:\n",
        "                parts = line.split(maxsplit=1)\n",
        "            if len(parts) == 2:\n",
        "                image_id, report = parts\n",
        "                captions = [caption.strip() for caption in report.split(\".\") if caption.strip()]\n",
        "                captions_dict[image_id] = captions\n",
        "            else:\n",
        "                print(f\"Skipping line due to unexpected format: {line.strip()}\")\n",
        "    return captions_dict\n",
        "\n",
        "def process_reports_in_groups(captions_dict, group_size=8):\n",
        "    grouped_captions_dict = {}\n",
        "    temp_dict = {}\n",
        "    for key, report in captions_dict.items():\n",
        "        prefix = key[:4]\n",
        "        if prefix not in temp_dict:\n",
        "            temp_dict[prefix] = []\n",
        "        temp_dict[prefix].append((key, report))\n",
        "    for prefix, items in temp_dict.items():\n",
        "        for i in range(0, len(items), group_size):\n",
        "            group = items[i:i + group_size]\n",
        "            if len(group) == group_size:\n",
        "                group_report = group[0][1]\n",
        "                for key, _ in group:\n",
        "                    grouped_captions_dict[key] = group_report\n",
        "    return grouped_captions_dict\n",
        "\n",
        "def extract_features(directory, image_keys):\n",
        "    model = VGG16()\n",
        "    model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
        "    features = dict()\n",
        "    for name in image_keys:\n",
        "        filename = os.path.join(directory, name + '.jpg')\n",
        "        image = load_img(filename, target_size=(224, 224))\n",
        "        image = img_to_array(image)\n",
        "        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "        image = preprocess_input(image)\n",
        "        feature = model.predict(image, verbose=0)\n",
        "        image_id = name.split('.')[0]\n",
        "        features[image_id] = feature\n",
        "    return features\n",
        "\n",
        "def load_large_pickle(file_path):\n",
        "    with open(file_path, 'rb') as file:\n",
        "        unpickler = pickle.Unpickler(file)\n",
        "        return unpickler.load()\n",
        "\n",
        "def data_generator(image_keys, Nmax, group_size=8, batch_size=1):\n",
        "    while True:\n",
        "        for i in range(0, len(image_keys), batch_size * group_size):\n",
        "            x1 = [[] for _ in range(group_size)]\n",
        "            x2, y = [[] for _ in range(Nmax)], [[] for _ in range(Nmax)]\n",
        "            batch_keys = image_keys[i:i + batch_size * group_size]\n",
        "            if len(batch_keys) < batch_size * group_size:\n",
        "                break\n",
        "            for j in range(0, len(batch_keys), group_size):\n",
        "                group_keys = batch_keys[j:j + group_size]\n",
        "                if len(group_keys) < group_size:\n",
        "                    break\n",
        "                group_features = [train_validate_features[image][0] for image in group_keys]\n",
        "                for image in group_keys:\n",
        "                    captions_list = train_validate_image_caption.get(image, [])\n",
        "                    for j, caption in enumerate(captions_list):\n",
        "                        seq = tokenizer.texts_to_sequences([caption.split()])[0]\n",
        "                        for k in range(1, len(seq)):\n",
        "                            x2_seq = pad_sequences([seq[:k]], maxlen=max_len)[0]\n",
        "                            y_seq = tf.keras.utils.to_categorical(seq[k], num_classes=vocab_len)\n",
        "                            for idx in range(group_size):\n",
        "                                x1[idx].append(group_features[idx])\n",
        "                            for m in range(Nmax):\n",
        "                                if m == j:\n",
        "                                    x2[m].append(x2_seq)\n",
        "                                    y[m].append(y_seq)\n",
        "                                else:\n",
        "                                    x2[m].append(np.zeros((max_len,), dtype=np.int32))\n",
        "                                    y[m].append(np.zeros((vocab_len,), dtype=np.float32))\n",
        "            yield (\n",
        "                tuple(np.array(x, dtype=np.float16) for x in x1) +\n",
        "                tuple(np.array(x, dtype=np.int32) for x in x2),\n",
        "                tuple(np.array(y_seq, dtype=np.float32) for y_seq in y)\n",
        "            )\n",
        "\n",
        "def define_model(max_len, vocab_size, Nmax, group_size=8):\n",
        "    input_images = [Input(shape=(4096,)) for _ in range(group_size)]\n",
        "    concatenated_images = Concatenate()(input_images)\n",
        "    image_features = Dense(2048, activation='relu', kernel_regularizer=l2(1e-4))(concatenated_images)\n",
        "    image_features = BatchNormalization()(image_features)\n",
        "    image_features = Dropout(0.4)(image_features)\n",
        "    input_captions = []\n",
        "    outputs = []\n",
        "    for i in range(Nmax):\n",
        "        input_caption = Input(shape=(max_len,))\n",
        "        input_captions.append(input_caption)\n",
        "        reshaped_input_caption = Reshape((max_len, 1))(input_caption)\n",
        "        lstm_seq = LSTM(128, return_sequences=True, recurrent_dropout=0.3, name=f'lstm_{i}')(reshaped_input_caption)\n",
        "        attention_output = Attention(name=f'attention_{i}')([lstm_seq, lstm_seq])\n",
        "        context_vector = GlobalAveragePooling1D()(attention_output)\n",
        "        context_vector = Dropout(0.4)(context_vector)\n",
        "        combined_features = Concatenate()([image_features, context_vector])\n",
        "        dense = Dense(128, activation='relu', kernel_regularizer=l2(1e-4))(combined_features)\n",
        "        dense = Dropout(0.3)(dense)\n",
        "        output = Dense(vocab_size, activation='softmax')(dense)\n",
        "        outputs.append(output)\n",
        "    model = tf.keras.Model(inputs=input_images + input_captions, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "def extract_features(image_paths):\n",
        "    base_model = VGG16()\n",
        "    #model = Model(inputs=base_model.input, outputs=model.layers[-2].output)\n",
        "    model = Model(inputs=base_model.input, outputs=base_model.get_layer('fc2').output)\n",
        "    features = {}\n",
        "    for image_path in image_paths:\n",
        "        image = load_img(image_path, target_size=(224, 224))\n",
        "        image = img_to_array(image)\n",
        "        image = preprocess_input(image)\n",
        "        image = np.expand_dims(image, axis=0)\n",
        "        feature = model.predict(image)\n",
        "        image_id = os.path.splitext(os.path.basename(image_path))[0]\n",
        "        features[image_id] = feature\n",
        "    return features\n",
        "\n",
        "def words_for_id(integer, tokenizers):\n",
        "    matching_words = []\n",
        "    for tokenizer in tokenizers:\n",
        "        if integer in tokenizer.word_index:\n",
        "            matching_words.append(tokenizer.index_word[integer])\n",
        "    return matching_words\n",
        "\n",
        "def generate_desc(model, tokenizers, photo, max_len, temperature=1.0):\n",
        "    all_predictions = []\n",
        "    for j, tokenizer in enumerate(tokenizers):\n",
        "        in_text = 'startseq'\n",
        "        predicted_words = []\n",
        "        for _ in range(max_len):\n",
        "            sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "            padded_sequence = pad_sequences([sequence], maxlen=max_len)\n",
        "            inputs = [photo] * 8\n",
        "            inputs += [np.zeros((1, max_len)) for _ in range(9)]\n",
        "            inputs[8 + j] = padded_sequence\n",
        "            yhat = model.predict(inputs, verbose=0)\n",
        "            yhat = yhat[j]\n",
        "            yhat = yhat.flatten()\n",
        "            yhat = np.log(yhat + 1e-10) / temperature\n",
        "            yhat = np.exp(yhat) / np.sum(np.exp(yhat))\n",
        "            next_index = np.random.choice(len(yhat), p=yhat)\n",
        "            next_word = tokenizer.index_word.get(next_index, None)\n",
        "            if next_word is not None and next_word != 'endseq':\n",
        "                predicted_words.append(next_word)\n",
        "                in_text += ' ' + next_word\n",
        "            else:\n",
        "                break\n",
        "        prediction = ' '.join(predicted_words).replace(' endseq', '').strip()\n",
        "        all_predictions.append(prediction)\n",
        "    return all_predictions\n",
        "\n",
        "def calculate_semantic_score(prediction):\n",
        "    inputs = bert_tokenizer(prediction, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = bert_model(**inputs, labels=inputs[\"input_ids\"])\n",
        "        loss = outputs.loss\n",
        "        perplexity = torch.exp(loss).item()\n",
        "    return -perplexity\n",
        "\n",
        "def calculate_heuristic_score(prediction):\n",
        "    if prediction.strip() == \"\":\n",
        "        return -float('inf')\n",
        "    return calculate_semantic_score(prediction)\n",
        "\n",
        "def clean_text(text):\n",
        "    if isinstance(text, str):\n",
        "        return re.sub(r'[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F]', '', text)\n",
        "    return text\n",
        "\n",
        "def calculate_bleu_scores(reference, hypothesis):\n",
        "    smoothing_function = SmoothingFunction().method1\n",
        "    bleu1 = sentence_bleu([reference], hypothesis, weights=(1, 0, 0, 0), smoothing_function=smoothing_function)\n",
        "    bleu2 = sentence_bleu([reference], hypothesis, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothing_function)\n",
        "    bleu3 = sentence_bleu([reference], hypothesis, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smoothing_function)\n",
        "    bleu4 = sentence_bleu([reference], hypothesis, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothing_function)\n",
        "    return bleu1, bleu2, bleu3, bleu4"
      ],
      "metadata": {
        "id": "Sg2M5cS3d54Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_dataset_path = ''\n",
        "caption_dataset_path = ''\n",
        "\n",
        "captions_dict = load_caption_file(caption_dataset_path)\n",
        "grouped_captions_dict = process_reports_in_groups(captions_dict)\n",
        "\n",
        "print(\"Number of images in the grouped_captions_dict dictionary:\", len(grouped_captions_dict))\n",
        "for image_id, captions in list(grouped_captions_dict.items()):\n",
        "    print(image_id, \":\", captions)\n",
        "print()\n",
        "\n",
        "new_captions_dict = {}\n",
        "table = str.maketrans('', '', string.punctuation)\n",
        "for caption_id, caption_list in grouped_captions_dict.items():\n",
        "    cleaned_captions = []\n",
        "    for caption_text in caption_list:\n",
        "        caption_text = caption_text.split()\n",
        "        caption_text = [token.lower() for token in caption_text]\n",
        "        caption_text = [token.translate(table) for token in caption_text]\n",
        "        caption_text = [token for token in caption_text if len(token) > 1]\n",
        "        cleaned_caption = ' '.join(caption_text)\n",
        "        cleaned_caption = 'startseq ' + cleaned_caption + ' endseq'\n",
        "        cleaned_captions.append(cleaned_caption)\n",
        "    new_captions_dict[caption_id] = cleaned_captions\n",
        "\n",
        "del grouped_captions_dict\n",
        "#gc.collect()\n",
        "\n",
        "Nmax = max(len(captions) for captions in new_captions_dict.values())\n",
        "Nmax"
      ],
      "metadata": {
        "id": "KY45g--3K1ah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "caption_images_list = []\n",
        "image_index = list(new_captions_dict.keys())\n",
        "caption_images_list = [image.split('.')[0] for image in os.listdir(image_dataset_path) if image.split('.')[0] in image_index]\n",
        "len(caption_images_list)\n",
        "\n",
        "prefix_groups = {}\n",
        "for image in caption_images_list:\n",
        "    prefix = image[:4]\n",
        "    if prefix not in prefix_groups:\n",
        "        prefix_groups[prefix] = []\n",
        "    prefix_groups[prefix].append(image)\n",
        "\n",
        "grouped_images = []\n",
        "for group in prefix_groups.values():\n",
        "    if len(group) == 8:\n",
        "        grouped_images.append(group)"
      ],
      "metadata": {
        "id": "uzVN5binEkcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(12)\n",
        "random.shuffle(grouped_images)\n",
        "flattened_images = [image for group in grouped_images for image in group]\n",
        "num_test_groups = int(0.20 * len(grouped_images))\n",
        "test_groups = grouped_images[:num_test_groups]\n",
        "train_validate_groups = grouped_images[num_test_groups:]\n",
        "test_images = [image for group in test_groups for image in group]\n",
        "train_validate_images = [image for group in train_validate_groups for image in group]\n",
        "print(f\"Number of training and validation slices: {len(train_validate_images)}\")\n",
        "print(f\"Number of test slices: {len(test_images)}\")\n",
        "random.shuffle(train_validate_images)\n",
        "random.shuffle(test_images)"
      ],
      "metadata": {
        "id": "CtU67np8GJL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "train_validate_features = extract_features(image_dataset_path, train_validate_images)\n",
        "with open(r'.../train-val-features.pkl', 'wb') as f: # .../ --> insert path where to save features\n",
        "    dump(train_validate_features, f)\n",
        "\n",
        "train_validate_features = load_large_pickle('.../train-val-features.pkl')\n",
        "len(train_validate_features)\n",
        "\n",
        "del captions_dict\n",
        "#gc.collect()\n",
        "\n",
        "train_validate_image_caption = {}\n",
        "for image, caption in new_captions_dict.items():\n",
        "    if image in train_validate_images and image in list(train_validate_features.keys()):\n",
        "        train_validate_image_caption.update({image: caption})\n",
        "len(train_validate_image_caption)"
      ],
      "metadata": {
        "id": "ttfFEWnV7I42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "all_captions = [caption for captions_list in new_captions_dict.values() for caption in captions_list]\n",
        "tokenizer.fit_on_texts(all_captions)\n",
        "vocab_len = len(tokenizer.word_index) + 1\n",
        "max_len = max(\n",
        "    max(len(caption.split()) for caption in captions_list) for captions_list in train_validate_image_caption.values())\n",
        "for caption in all_captions[:6]:\n",
        "    sequence = tokenizer.texts_to_sequences([caption])[0]\n",
        "    print(\"Original caption:\", caption)\n",
        "    print(\"Tokenized sequence:\", sequence)\n",
        "    print()"
      ],
      "metadata": {
        "id": "1oQniZtOHpXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "policy = mixed_precision.Policy('mixed_float16')\n",
        "mixed_precision.set_global_policy(policy)\n",
        "\n",
        "total_train_validate = len(train_validate_images)\n",
        "num_validate_images = int(0.15 * total_train_validate)\n",
        "num_train_images = total_train_validate - num_validate_images\n",
        "group_size = 8\n",
        "batch_size = 32\n",
        "\n",
        "train_tf_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: data_generator(train_validate_images[:num_train_images], Nmax, group_size=group_size, batch_size=batch_size),\n",
        "    output_signature=(\n",
        "        tuple(tf.TensorSpec(shape=(None, 4096), dtype=tf.float16) for _ in range(8)) +\n",
        "        tuple(tf.TensorSpec(shape=(None, max_len), dtype=tf.int32) for _ in range(Nmax)),\n",
        "        tuple(tf.TensorSpec(shape=(None, vocab_len), dtype=tf.float32) for _ in range(Nmax))\n",
        "    )\n",
        ").prefetch(tf.data.AUTOTUNE)\n",
        "#).prefetch(2) # reduce prefetch to limit memory usage\n",
        "\n",
        "validate_tf_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: data_generator(train_validate_images[num_train_images:], Nmax, group_size=group_size, batch_size=batch_size),\n",
        "    output_signature=(\n",
        "        tuple(tf.TensorSpec(shape=(None, 4096), dtype=tf.float16) for _ in range(8)) +\n",
        "        tuple(tf.TensorSpec(shape=(None, max_len), dtype=tf.int32) for _ in range(Nmax)),\n",
        "        tuple(tf.TensorSpec(shape=(None, vocab_len), dtype=tf.float32) for _ in range(Nmax))\n",
        "    )\n",
        ").prefetch(tf.data.AUTOTUNE)\n",
        "#).prefetch(2) # reduce prefetch to limit memory usage"
      ],
      "metadata": {
        "id": "9YGnBvni02Br"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = vocab_len\n",
        "model = define_model(max_len, vocab_size, Nmax, group_size=8)\n",
        "optimizer = Adam(learning_rate=0.0001, clipnorm=1.0)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy')\n",
        "\n",
        "checkpoint_cb = ModelCheckpoint(\n",
        "    '.../checkpoint-with-attention-EN.weights.h5', # .../ --> insert path where to save model checkpoints\n",
        "    save_best_only=False,\n",
        "    save_weights_only=True,\n",
        "    save_freq='epoch'\n",
        ")\n",
        "earlystop_cb = EarlyStopping(patience=5, restore_best_weights=True)\n",
        "callbacks = [checkpoint_cb, earlystop_cb]\n",
        "history = model.fit(\n",
        "    train_tf_dataset,\n",
        "    epochs=100,\n",
        "    validation_data=validate_tf_dataset,\n",
        "    steps_per_epoch=num_train_images // batch_size,\n",
        "    validation_steps=num_validate_images // batch_size,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "model.save('.../model-weights-with-attention-EN.h5') # .../ --> insert path where to save model weights"
      ],
      "metadata": {
        "id": "yQI-GtKYtzef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \".../history-with-attention-EN.txt\"\n",
        "\n",
        "loss = []\n",
        "val_loss = []\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.readlines()\n",
        "for line in lines:\n",
        "    train_loss_match = re.search(r\" loss: ([0-9]+\\.[0-9]+) \", line)\n",
        "    val_loss_match = re.search(r\"val_loss: ([0-9]+\\.[0-9]+)\", line)\n",
        "    if train_loss_match and val_loss_match:\n",
        "        loss.append(float(train_loss_match.group(1)))\n",
        "        val_loss.append(float(val_loss_match.group(1)))\n",
        "print(\"Epochs:\", len(loss))\n",
        "print(\"Initial loss:\", loss[:5])\n",
        "print(\"Initial val loss:\", val_loss[:5])\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(range(1, len(loss) + 1), loss, label='TRAINING LOSS')\n",
        "plt.plot(range(1, len(val_loss) + 1), val_loss, label='VALIDATION LOSS')\n",
        "if val_loss:\n",
        "    min_epoch = val_loss.index(min(val_loss)) + 1\n",
        "    plt.scatter(min_epoch, min(val_loss), color='red', zorder=5, label=f'MIN VAL LOSS (epoch {min_epoch})')\n",
        "plt.xlabel('EPOCH')\n",
        "plt.ylabel('LOSS')\n",
        "plt.title('LEARNING CURVES')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lXf1twLm8NgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = define_model(max_len, vocab_size, Nmax, group_size=8)\n",
        "model.load_weights('.../model-weights-with-attention-EN.h5')\n",
        "for i, layer in enumerate(model.inputs):\n",
        "    print(f\"Input {i+1}: {layer.shape}\")\n",
        "\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "bert_model = AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizers = [Tokenizer() for _ in range(9)]\n",
        "for tokenizer in tokenizers:\n",
        "    tokenizer.fit_on_texts([caption for _, captions in new_captions_dict.items() for caption in captions])\n",
        "\n",
        "predicted_captions = []\n",
        "actual_captions = []\n",
        "image_names = []\n",
        "csv_file_path = '.../output-semantic-with-attention-EN.csv' # .../ --> insert path where to save csv file\n",
        "with open(csv_file_path, 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['Image Prefix', 'Predicted Report', 'Actual Report'])\n",
        "    prefix_groups = {}\n",
        "    for image in test_images:\n",
        "        prefix = image[:4]\n",
        "        if prefix not in prefix_groups:\n",
        "            prefix_groups[prefix] = []\n",
        "        prefix_groups[prefix].append(image)\n",
        "    for prefix, images in prefix_groups.items():\n",
        "        print(f\"Frames of CT scan number: {prefix}\")\n",
        "        all_captions_list = [new_captions_dict[image] for image in images]\n",
        "        image_paths = [os.path.join(image_dataset_path, image + '.jpg') for image in images]\n",
        "        image_features = extract_features(image_paths)\n",
        "        photos = [np.array([image_features[image][0]]) for image in images]\n",
        "        group_predictions = [[] for _ in range(9)]\n",
        "        for photo, image in zip(photos, images):\n",
        "            predictions = generate_desc(model, tokenizers, photo, max_len)\n",
        "            captions_list = new_captions_dict[image]\n",
        "            num_actual_captions = len(captions_list)\n",
        "            if num_actual_captions < 9:\n",
        "                for i in range(num_actual_captions, 9):\n",
        "                    predictions[i] = \"\"\n",
        "            for i in range(9):\n",
        "                if i < len(predictions):\n",
        "                    group_predictions[i].append(predictions[i])\n",
        "            for j, prediction in enumerate(predictions):\n",
        "                if j < num_actual_captions:\n",
        "                    print(f\"Prediction for predictor {j + 1}: {prediction}\")\n",
        "                else:\n",
        "                    print(f\"Prediction for predictor {j + 1}: (empty)\")\n",
        "            print('---')\n",
        "        best_predictions = [\"\"] * 9\n",
        "        best_scores = [-1] * 9\n",
        "        for i in range(9):\n",
        "            best_prediction_for_position = None\n",
        "            found_valid_prediction = False\n",
        "            for prediction in group_predictions[i]:\n",
        "                if not prediction.strip():\n",
        "                    continue\n",
        "                found_valid_prediction = True\n",
        "                heuristic_score = calculate_heuristic_score(prediction)\n",
        "                if heuristic_score > best_scores[i]:\n",
        "                    best_scores[i] = heuristic_score\n",
        "                    best_prediction_for_position = prediction\n",
        "            if not found_valid_prediction or best_prediction_for_position is None:\n",
        "                if group_predictions[i]:\n",
        "                    best_prediction_for_position = group_predictions[i][0]\n",
        "                else:\n",
        "                    best_prediction_for_position = \"\"\n",
        "            best_predictions[i] = best_prediction_for_position\n",
        "        predicted_captions.append(best_predictions)\n",
        "        actual_captions.append([word for word in all_captions_list[0][0].split() if word not in ['startseq', 'endseq']])\n",
        "        print()\n",
        "        print(f\"Best prediction for frames of CT scan number {prefix}: {best_predictions}\")\n",
        "        print()\n",
        "        print(\"Predicted -> \", best_predictions)\n",
        "        print(\"Actual -> \", [' '.join([word for word in caption.split() if word not in ['startseq', 'endseq']]) for caption in all_captions_list[0]])\n",
        "        print('*********************************************************************')\n",
        "        print()\n",
        "        image_names.append(prefix)\n",
        "        filtered_preds = [element for element in best_predictions if element]\n",
        "        best_preds = \" \".join(filtered_preds)\n",
        "        actuals = \" \".join([' '.join([word for word in caption.split() if word not in ['startseq', 'endseq']]) for caption in all_captions_list[0]])\n",
        "        writer.writerow([prefix, best_preds, actuals])"
      ],
      "metadata": {
        "id": "B4nm5XFvVNmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_file_path = '.../output-semantic-with-attention-EN.csv'\n",
        "df = pd.read_csv(csv_file_path)\n",
        "\n",
        "file_path = '.../output-semantic-with-attention-EN.xlsx' # .../ --> insert path where to save xlsx file\n",
        "df = df.map(clean_text)\n",
        "df.to_excel(file_path, index=False, engine='openpyxl')\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "rouge = Rouge()\n",
        "bleu1_scores = []\n",
        "bleu2_scores = []\n",
        "bleu3_scores = []\n",
        "bleu4_scores = []\n",
        "rouge1_scores = []\n",
        "rouge2_scores = []\n",
        "rougeL_scores = []\n",
        "meteor_scores = []\n",
        "for index, row in df.iterrows():\n",
        "    reference = row['Actual Report'].split()\n",
        "    hypothesis = row['Predicted Report'].split()\n",
        "    bleu1, bleu2, bleu3, bleu4 = calculate_bleu_scores(reference, hypothesis)\n",
        "    bleu1_scores.append(bleu1)\n",
        "    bleu2_scores.append(bleu2)\n",
        "    bleu3_scores.append(bleu3)\n",
        "    bleu4_scores.append(bleu4)\n",
        "    rouge_scores = rouge.get_scores(' '.join(hypothesis), ' '.join(reference))[0]\n",
        "    rouge1_scores.append(rouge_scores['rouge-1']['f'])\n",
        "    rouge2_scores.append(rouge_scores['rouge-2']['f'])\n",
        "    rougeL_scores.append(rouge_scores['rouge-l']['f'])\n",
        "    meteor = meteor_score([reference], hypothesis)\n",
        "    meteor_scores.append(meteor)\n",
        "\n",
        "df['BLEU-1'] = bleu1_scores\n",
        "df['BLEU-2'] = bleu2_scores\n",
        "df['BLEU-3'] = bleu3_scores\n",
        "df['BLEU-4'] = bleu4_scores\n",
        "df['ROUGE-1'] = rouge1_scores\n",
        "df['ROUGE-2'] = rouge2_scores\n",
        "df['ROUGE-L'] = rougeL_scores\n",
        "df['METEOR'] = meteor_scores\n",
        "global_bleu1 = df['BLEU-1'].mean()\n",
        "global_bleu2 = df['BLEU-2'].mean()\n",
        "global_bleu3 = df['BLEU-3'].mean()\n",
        "global_bleu4 = df['BLEU-4'].mean()\n",
        "global_rouge1 = df['ROUGE-1'].mean()\n",
        "global_rouge2 = df['ROUGE-2'].mean()\n",
        "global_rougeL = df['ROUGE-L'].mean()\n",
        "global_meteor = df['METEOR'].mean()\n",
        "df.loc['Global Average'] = [''] * (len(df.columns) - 8) + [global_bleu1, global_bleu2, global_bleu3, global_bleu4, global_rouge1, global_rouge2, global_rougeL, global_meteor]\n",
        "\n",
        "output_file_path = '.../output-semantic&scores-with-attention-EN.xlsx' # insert path where to save xlsx file\n",
        "\n",
        "df.to_excel(output_file_path, index=False)\n",
        "wb = openpyxl.load_workbook(output_file_path)\n",
        "ws = wb.active\n",
        "last_row = ws.max_row\n",
        "red_font = Font(color=\"FF0000\", bold=True)\n",
        "for cell in ws[last_row]:\n",
        "    cell.font = red_font\n",
        "wb.save(output_file_path)"
      ],
      "metadata": {
        "id": "yydCJKVHqkLQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}